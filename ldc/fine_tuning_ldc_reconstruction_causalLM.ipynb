{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8c713e05-3adb-4610-8ebf-4671498a8d82",
   "metadata": {},
   "source": [
    "## Fine-tuning Llama 3.1 (QLoRA) on LDC regeneration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9e592cac-280d-4c2c-8919-30759fb16346",
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "import torch\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import sentence_transformers\n",
    "import nltk\n",
    "import glob\n",
    "import datasets\n",
    "import peft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dbcbb3c7-01b3-4155-ac34-e38ddfa04518",
   "metadata": {},
   "outputs": [],
   "source": [
    "cache_directory = '/scratch/alpine/anra7539' # For caching model init\n",
    "checkpoint_directory = \"/scratch/alpine/anra7539/llama3.1_qlora_finetuned\" # For fine-tuned model checkpoint\n",
    "inference_results = '/projects/anra7539/projects/representation_efficacy/ldc_reconstructions_llama3.1_qlora/reconstructed_text.json' # For storing inference output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecfcc0f6-29e4-4d4f-9fc9-295f754adfa9",
   "metadata": {},
   "source": [
    "## Preparing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c36028d6-394a-40a7-bb6f-fbfab3a28226",
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_path_train = \"/projects/anra7539/projects/representation_efficacy/ldc_data_train/\"\n",
    "folder_path_val = \"/projects/anra7539/projects/representation_efficacy/ldc_data_val/\"\n",
    "folder_path_test = \"/projects/anra7539/projects/representation_efficacy/ldc_data_test/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "79a948ac-7f5b-47ba-87f9-39080d59bc42",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = []\n",
    "val_data = []\n",
    "test_data = []\n",
    "\n",
    "for file_path in os.listdir(folder_path_train):  \n",
    "    with open(folder_path_train+file_path, \"r\") as f:\n",
    "        data = f.read()\n",
    "    train_data.append(data)\n",
    "\n",
    "for file_path in os.listdir(folder_path_val):  \n",
    "    with open(folder_path_val+file_path, \"r\") as f:\n",
    "        data = f.read()\n",
    "    val_data.append(data)\n",
    "\n",
    "for file_path in os.listdir(folder_path_test):  \n",
    "    with open(folder_path_test+file_path, \"r\") as f:\n",
    "        data = f.read()\n",
    "    test_data.append(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fc6fa51c-e21c-4c76-a44d-93fd7028b3ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_amr_pairs_train = sum([a.split(\"::snt\")[1:] for a in train_data], [])\n",
    "text_amr_pairs_val = sum([a.split(\"::snt\")[1:] for a in val_data], [])\n",
    "text_amr_pairs_test = sum([a.split(\"::snt\")[1:] for a in test_data], [])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a368c7eb-48f6-4137-ac84-4398cf5b00eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 62238/62238 [00:00<00:00, 176098.87it/s]\n"
     ]
    }
   ],
   "source": [
    "text_amr_train = []\n",
    "\n",
    "for i in tqdm(range(len(text_amr_pairs_train))):\n",
    "    try:\n",
    "        text_amr_train.append([text_amr_pairs_train[i].split(\"\\n#\")[0].strip(), text_amr_pairs_train[i].split(\"\\n#\")[1].split(\".txt\\n\")[1].strip()])\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "texts_train = []\n",
    "amrs_train = []\n",
    "for text,amr in text_amr_train:\n",
    "    texts_train.append(text)\n",
    "    amrs_train.append(amr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "67635185-ed7d-4247-8882-5967127b613d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2548/2548 [00:00<00:00, 713328.43it/s]\n"
     ]
    }
   ],
   "source": [
    "text_amr_val = []\n",
    "\n",
    "for i in tqdm(range(len(text_amr_pairs_val))):\n",
    "    try:\n",
    "        text_amr_val.append([text_amr_pairs_val[i].split(\"\\n#\")[0].strip(), text_amr_pairs_val[i].split(\"\\n#\")[1].split(\".txt\\n\")[1].strip()])\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "texts_val = []\n",
    "amrs_val = []\n",
    "for text,amr in text_amr_val:\n",
    "    texts_val.append(text)\n",
    "    amrs_val.append(amr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "658012da-a7b4-4a5c-ba9f-d23439591d08",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2721/2721 [00:00<00:00, 664185.60it/s]\n"
     ]
    }
   ],
   "source": [
    "text_amr_test = []\n",
    "\n",
    "for i in tqdm(range(len(text_amr_pairs_test))):\n",
    "    try:\n",
    "        text_amr_test.append([text_amr_pairs_test[i].split(\"\\n#\")[0].strip(), text_amr_pairs_test[i].split(\"\\n#\")[1].split(\".txt\\n\")[1].strip()])\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "texts_test = []\n",
    "amrs_test = []\n",
    "for text,amr in text_amr_test:\n",
    "    texts_test.append(text)\n",
    "    amrs_test.append(amr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d27a61d5-fce3-4972-8cdf-e23da9e1f638",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = datasets.Dataset.from_pandas(pd.DataFrame({'Text':texts_train, 'amr':amrs_train}))\n",
    "test_df = datasets.Dataset.from_pandas(pd.DataFrame({'Text':texts_test, 'amr':amrs_test}))\n",
    "val_df = datasets.Dataset.from_pandas(pd.DataFrame({'Text':texts_val, 'amr':amrs_val}))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e0c7c50-9762-420b-9b63-ff705d6b9c0b",
   "metadata": {},
   "source": [
    "## Initializing model and tokenizing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f2089fcb-6874-4944-a25b-3b3ae6c28106",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "80d9616e334f40158665cac0baa047d5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "name = \"meta-llama/Llama-3.1-8B-Instruct\"\n",
    "device = \"cuda:0\"\n",
    "\n",
    "model = transformers.AutoModelForCausalLM.from_pretrained(name,\n",
    "                                                          load_in_8bit = True,\n",
    "                                                          trust_remote_code = True,\n",
    "                                              device_map={\"\": torch.cuda.current_device()},\n",
    "                                             cache_dir=cache_directory)\n",
    "\n",
    "tokenizer = transformers.AutoTokenizer.from_pretrained(name, truncation_side = \"left\")\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.pad_token_id = tokenizer.eos_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9c648f2e-089a-40df-9392-33c3eabab6f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_function(example):\n",
    "    start_prompt = 'Return the original text of the given Abstract Meaning Representation (AMR) structure.\\n\\nAMR:\\n'\n",
    "    end_prompt = '\\n\\nText: '\n",
    "    \n",
    "    full_texts = []\n",
    "    prompt_texts = []\n",
    "    \n",
    "    for amr, text in zip(example[\"amr\"], example[\"Text\"]):\n",
    "        prompt_text = start_prompt + amr + end_prompt\n",
    "        full_text = prompt_text + text\n",
    "        prompt_texts.append(prompt_text)\n",
    "        full_texts.append(full_text)\n",
    "    \n",
    "    encoding = tokenizer(\n",
    "        full_texts,\n",
    "        return_tensors=\"pt\",\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        max_length=1024\n",
    "    )\n",
    "    \n",
    "    input_ids = encoding.input_ids.to(device)\n",
    "    labels = input_ids.clone()\n",
    "    \n",
    "    for i, prompt in enumerate(prompt_texts):\n",
    "        prompt_encoding = tokenizer(\n",
    "            prompt,\n",
    "            return_tensors=\"pt\",\n",
    "            truncation=True,\n",
    "            max_length=1024\n",
    "        )\n",
    "        prompt_len = prompt_encoding.input_ids.shape[1]\n",
    "        labels[i, :prompt_len] = -100\n",
    "    \n",
    "    example[\"input_ids\"] = input_ids\n",
    "    example[\"labels\"] = labels\n",
    "    return example\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3268a3a7-cc30-47c8-87bc-8c0e7d9b4798",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f2e33480351c408ab916931d88029dc6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/55635 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "965960bf41a14764bda1709727ed1bb1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1722 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "81b3577128794eb6af5a207e287b2fea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1898 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "batch_size = 8\n",
    "\n",
    "tokenized_datasets_train = train_df.map(tokenize_function, batched=True, batch_size = batch_size)\n",
    "tokenized_datasets_val = val_df.map(tokenize_function, batched=True, batch_size = batch_size)\n",
    "tokenized_datasets_test = test_df.map(tokenize_function, batched=True, batch_size = batch_size)\n",
    "\n",
    "tokenized_datasets_train = tokenized_datasets_train.remove_columns(['Text', 'amr'])\n",
    "tokenized_datasets_val = tokenized_datasets_val.remove_columns(['Text', 'amr'])\n",
    "tokenized_datasets_test = tokenized_datasets_test.remove_columns(['Text', 'amr'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f1db0ab-5624-4745-a060-9ce0d59abc53",
   "metadata": {},
   "source": [
    "## QLoRA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "147a13dc-3542-4908-93f7-f0032532d556",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_number_of_trainable_model_parameters(model):\n",
    "    trainable_model_params = 0\n",
    "    all_model_params = 0\n",
    "    for _, param in model.named_parameters():\n",
    "        all_model_params += param.numel()\n",
    "        if param.requires_grad:\n",
    "            trainable_model_params += param.numel()\n",
    "    return f\"trainable model parameters: {trainable_model_params}\\nall model parameters: {all_model_params}\\npercentage of trainable model parameters: {100 * trainable_model_params / all_model_params:.2f}%\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2caa2851-a415-45ba-8b6c-d64009d8602f",
   "metadata": {},
   "outputs": [],
   "source": [
    "lora_config = peft.LoraConfig(\n",
    "    r=32,\n",
    "    lora_alpha=32,\n",
    "    target_modules=[\"q_proj\", \"v_proj\"],\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=peft.TaskType.CAUSAL_LM \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7c0c95eb-48c8-446b-8b56-bccaec63c4d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable model parameters: 13631488\n",
      "all model parameters: 8043892736\n",
      "percentage of trainable model parameters: 0.17%\n"
     ]
    }
   ],
   "source": [
    "peft_model = peft.get_peft_model(model, \n",
    "                            lora_config)\n",
    "print(print_number_of_trainable_model_parameters(peft_model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5261152c-b099-4ee6-8d92-db308d20259a",
   "metadata": {},
   "outputs": [],
   "source": [
    "peft_training_args = transformers.TrainingArguments(\n",
    "    output_dir=checkpoint_directory,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    learning_rate=1e-3,\n",
    "    num_train_epochs=10,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    logging_steps=10,\n",
    "    metric_for_best_model=\"eval_loss\",\n",
    "    load_best_model_at_end = True,\n",
    "    greater_is_better=False,\n",
    "    save_total_limit=1,\n",
    ")\n",
    "\n",
    "\n",
    "peft_trainer = transformers.Trainer(\n",
    "    model=peft_model,\n",
    "    args=peft_training_args,\n",
    "    train_dataset=tokenized_datasets_train,\n",
    "    eval_dataset=tokenized_datasets_val,\n",
    "    callbacks=[transformers.EarlyStoppingCallback(early_stopping_patience=3)],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "52ef60bf-76b7-4248-9a76-0e681b6fb527",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "peft_trainer.train()\n",
    "\n",
    "peft_model_path=checkpoint_directory\n",
    "\n",
    "peft_trainer.model.save_pretrained(peft_model_path)\n",
    "tokenizer.save_pretrained(peft_model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e980dd81-6c22-477f-99b2-ecf4f8961839",
   "metadata": {},
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ae5bf440-b703-43a0-8dbe-f4cc94e73d8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3a864445b96f44b8b08edd7b35c8bcca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "base_model = transformers.AutoModelForCausalLM.from_pretrained(name,\n",
    "                                                          load_in_8bit = True,\n",
    "                                                          trust_remote_code = True,\n",
    "                                              device_map={\"\": torch.cuda.current_device()},\n",
    "                                             cache_dir=cache_directory)\n",
    "tokenizer = transformers.AutoTokenizer.from_pretrained(name, truncation_side = \"left\")\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "\n",
    "peft_model = peft.PeftModel.from_pretrained(base_model, \n",
    "                                       checkpoint_directory,\n",
    "                                       is_trainable=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "fcfd5f48-9880-4a47-ac79-3d8ead8071b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reconstruct_text(amr, prompt, model, tokenizer):\n",
    "    with torch.no_grad():\n",
    "        input_text = f'''{prompt}\\n\\nAMR:\\n{amr}\\n\\nText:'''\n",
    "        input_tokens = tokenizer(input_text, return_tensors = \"pt\", truncation = True, \n",
    "                                 padding = 'max_length', max_length = 1024).to(device)\n",
    "    \n",
    "        outputs = model.generate(**input_tokens, max_new_tokens = 200, \n",
    "                                 pad_token_id = tokenizer.eos_token_id)\n",
    "    \n",
    "        reconstruction = tokenizer.decode(outputs[0], \n",
    "                                          skip_special_tokens = True).split(\"Text:\\n\")[1].split(\"\\n\")[0].strip()\n",
    "    return reconstruction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "934fd010-7bc2-4430-8238-7cdace0faa9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = 'Return the original text of the given Abstract Meaning Representation (AMR) structure.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaa6b93a-ae3b-4b44-ad78-1684583a0f5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_file = inference_results\n",
    "\n",
    "if os.path.exists(output_file):\n",
    "    with open(output_file, 'r') as f:\n",
    "        try:\n",
    "            existing_data = [json.loads(line) for line in f]\n",
    "        except json.JSONDecodeError:\n",
    "            existing_data = []\n",
    "else:\n",
    "    existing_data = []\n",
    "\n",
    "processed_indices = {item['index'] for item in existing_data}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c74c812-ffc0-4ad3-a596-6bcf6ad1e3e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(output_file, 'a') as f:\n",
    "    for i in tqdm(range(len(amrs_test))):\n",
    "        if i in processed_indices:\n",
    "            continue \n",
    "        \n",
    "        reconstructed_text = reconstruct_text(amrs_test[i], prompt, peft_model, tokenizer)\n",
    "        \n",
    "        result = {\n",
    "            \"index\": i,\n",
    "            \"original_text\": texts[i],\n",
    "            \"reconstructed_text\": reconstructed_text\n",
    "        }\n",
    "\n",
    "        f.write(json.dumps(result) + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3e13741-f5e3-4838-9152-241f2b665eae",
   "metadata": {},
   "source": [
    "## Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e83b890a-339f-4888-a5b0-2e3dc8925368",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(inference_results, 'r') as f:\n",
    "    data = [json.loads(line) for line in f]\n",
    "\n",
    "full_reconstructions = pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "884415c8-e81e-49fa-ac36-d9859452c699",
   "metadata": {},
   "source": [
    "### Average F1-score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b015e275-974e-42c3-b384-67ef369e5f0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def f1_score_strings(str1, str2):\n",
    "    tokens1 = set(str1.lower().split())\n",
    "    tokens2 = set(str2.lower().split())\n",
    "    \n",
    "    true_positives = len(tokens1 & tokens2)  \n",
    "    false_positives = len(tokens1 - tokens2)  \n",
    "    false_negatives = len(tokens2 - tokens1)  \n",
    "    \n",
    "    precision = true_positives / (true_positives + false_positives) if true_positives + false_positives > 0 else 0\n",
    "    recall = true_positives / (true_positives + false_negatives) if true_positives + false_negatives > 0 else 0\n",
    "    \n",
    "    if precision + recall > 0:\n",
    "        f1 = 2 * (precision * recall) / (precision + recall)\n",
    "    else:\n",
    "        f1 = 0\n",
    "    \n",
    "    return f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7be8107-44f3-4c61-aeac-7ea6b5eb6b15",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_reconstructions['f1_scores'] = full_reconstructions.apply(lambda x: f1_score_strings(x['original_text'], x['reconstructed_text']), axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6c106bb-ce89-4cf3-9576-676f9699c844",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(full_reconstructions.f1_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8647450-5e60-436c-a84b-fa2517ec2910",
   "metadata": {},
   "source": [
    "### Cosine Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96666032-ab0e-474f-b57d-5c47f245fca8",
   "metadata": {},
   "outputs": [],
   "source": [
    "similarity_model = sentence_transformers.SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "\n",
    "def sent_similarity(str1, str2):\n",
    "    embedding1 = similarity_model.encode(str1.lower())\n",
    "    embedding2 = similarity_model.encode(str2.lower())\n",
    "    \n",
    "    return sentence_transformers.util.cos_sim(embedding1, embedding2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c44ea548-99eb-476a-84c7-bf17604bda4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_reconstructions['cosine_similarity'] = full_reconstructions.apply(lambda x: sent_similarity(x['original_text'], x['reconstructed_text']), axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "371d4c57-07dc-4fdd-af4a-afe97c17eadd",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(full_reconstructions.cosine_similarity)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cc15aaf-7eda-4b62-aa2d-f9d698bc7fd8",
   "metadata": {},
   "source": [
    "### ROUGE scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5173b84-1211-45b0-94a9-c0ff8823aa49",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_rouge_scores(reference_text, generated_text):\n",
    "    scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
    "    scores = scorer.score(reference_text.lower(), generated_text.lower())\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57658959-cd4e-4e71-9b44-9e5a227adeda",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_reconstructions['rouge_scores'] = full_reconstructions.apply(lambda x: compute_rouge_scores(x['original_text'], x['reconstructed_text']), axis = 1)\n",
    "full_reconstructions['rouge_1'] = full_reconstructions.rouge_scores.map(lambda x: x['rouge1'].fmeasure)\n",
    "full_reconstructions['rouge_2'] = full_reconstructions.rouge_scores.map(lambda x: x['rouge2'].fmeasure)\n",
    "full_reconstructions['rouge_l'] = full_reconstructions.rouge_scores.map(lambda x: x['rougeL'].fmeasure)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91d485c6-55c4-4d9e-ae80-6b3863c22e08",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"ROUGE-1 score = {np.mean(full_reconstructions.rouge_1)}\")\n",
    "print(f\"ROUGE-2 score = {np.mean(full_reconstructions.rouge_2)}\")\n",
    "print(f\"ROUGE-L score = {np.mean(full_reconstructions.rouge_l)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e759785-bdf1-4ab1-a179-e0b4775ba1d7",
   "metadata": {},
   "source": [
    "### BLEU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fcbe60b-8dd6-404d-8768-3ca474c2e137",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.translate.bleu_score import sentence_bleu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6c033ec-a7f8-4485-9eac-e887e2695360",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_reconstructions['bleu_scores'] = full_reconstructions.apply(lambda x: sentence_bleu([nltk.word_tokenize(x['original_text'].lower())], \n",
    "                                                                                         nltk.word_tokenize(x['reconstructed_text'].lower())), axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9433ef40-9167-4a7d-8819-b262d0e1f751",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.mean(full_reconstructions.bleu_scores))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
