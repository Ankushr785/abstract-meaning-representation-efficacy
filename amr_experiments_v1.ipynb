{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3439bce3-2a36-47c9-8134-c2624c160288",
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "import torch\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import sentence_transformers\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e928822a-88b7-4e8e-9a21-e1733e9dabe2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.device_count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b92a0f46-291b-42f8-b350-6e3a4dc88816",
   "metadata": {},
   "source": [
    "## Obtaining reconstructions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "04678b07-45d8-417d-b52a-543da5ce60b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('amr-release-3.0-amrs-bolt.txt', \"r\") as f:\n",
    "  data = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fd8c3847-eb10-4cf7-90dc-906c18559342",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_amr_pairs = data.split(\"::snt\")[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d77bd3bb-4c24-4fc4-83dd-6efb73011ae6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1327/1327 [00:00<00:00, 431092.98it/s]\n"
     ]
    }
   ],
   "source": [
    "texts = []\n",
    "amrs = []\n",
    "\n",
    "for i in tqdm(range(len(text_amr_pairs))):\n",
    "    try:\n",
    "        texts.append(text_amr_pairs[i].split(\"\\n#\")[0].strip())\n",
    "        amrs.append(text_amr_pairs[i].split(\"\\n#\")[1].split(\".txt\\n\")[1].strip())\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "979a5b3f-f1ad-41a7-9812-a5ad96dd4ed8",
   "metadata": {},
   "outputs": [],
   "source": [
    "name = \"meta-llama/Meta-Llama-3-8B-Instruct\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b0c7cf2a-a919-440a-8a9a-66d284b488b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "74bb5e6afc0d4e0b8fb90e442c19dd58",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "device = \"cuda\"\n",
    "\n",
    "model = transformers.AutoModelForCausalLM.from_pretrained(name,\n",
    "                                                          load_in_8bit = True,\n",
    "                                                          trust_remote_code = True,\n",
    "                                                          torch_dtype = torch.bfloat16,\n",
    "                                             device_map = device,\n",
    "                                             cache_dir='/scratch/alpine/anra7539')\n",
    "\n",
    "tokenizer = transformers.AutoTokenizer.from_pretrained(name, padding_side=\"left\", truncation_side = \"left\")\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.pad_token_id = tokenizer.eos_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f61e32a3-788c-4be7-b911-bf56be19413c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reconstruct_text(amr, prompt):\n",
    "    with torch.no_grad():\n",
    "        input_text = f'''{prompt}\\nAMR:{amr}\\nText:'''\n",
    "        input_tokens = tokenizer(input_text, return_tensors = \"pt\", truncation = True, max_length = 4096).to(device)\n",
    "    \n",
    "        outputs = model.generate(**input_tokens, max_new_tokens = 200, pad_token_id = tokenizer.eos_token_id)\n",
    "    \n",
    "        answer = tokenizer.decode(outputs[0], skip_special_tokens = True).split(\"Text:\")[11].split(\"\\n\")[0].strip()\n",
    "    return answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "71ca369e-2832-4ad0-9730-3c45a1980028",
   "metadata": {},
   "outputs": [],
   "source": [
    "few_shot_examples = np.random.randint(1000, size = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7a99b1aa-c547-4367-85dd-fa154d624658",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "examples = \"\\n\".join([f\"AMR:{amrs[i]}\\nText:{texts[i]}\" for i in few_shot_examples])\n",
    "\n",
    "prompt = f'''Based on the 5 examples below, return the original text of the given AMR:\\n\\n{examples}'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5600c769-30e9-4ef1-a70b-971b3872348a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1027 [00:00<?, ?it/s]/projects/anra7539/software/anaconda/envs/kgenv/lib/python3.11/site-packages/bitsandbytes/autograd/_functions.py:316: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      "Starting from v4.46, the `logits` model output will have the same type as the model (except at train time, where it will always be FP32)\n",
      " 44%|████▍     | 450/1027 [3:18:59<4:15:09, 26.53s/it]\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m reconstructed_texts \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m tqdm(amrs[\u001b[38;5;241m300\u001b[39m:]):\n\u001b[0;32m----> 3\u001b[0m     reconstructed_texts\u001b[38;5;241m.\u001b[39mappend(reconstruct_text(i, prompt))\n",
      "Cell \u001b[0;32mIn[8], line 8\u001b[0m, in \u001b[0;36mreconstruct_text\u001b[0;34m(amr, prompt)\u001b[0m\n\u001b[1;32m      4\u001b[0m     input_tokens \u001b[38;5;241m=\u001b[39m tokenizer(input_text, return_tensors \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m, truncation \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m, max_length \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m4096\u001b[39m)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m      6\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mgenerate(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39minput_tokens, max_new_tokens \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m200\u001b[39m, pad_token_id \u001b[38;5;241m=\u001b[39m tokenizer\u001b[38;5;241m.\u001b[39meos_token_id)\n\u001b[0;32m----> 8\u001b[0m     answer \u001b[38;5;241m=\u001b[39m tokenizer\u001b[38;5;241m.\u001b[39mdecode(outputs[\u001b[38;5;241m0\u001b[39m], skip_special_tokens \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m)\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mText:\u001b[39m\u001b[38;5;124m\"\u001b[39m)[\u001b[38;5;241m11\u001b[39m]\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mstrip()\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m answer\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "reconstructed_texts = []\n",
    "for i in tqdm(amrs[300:]):\n",
    "    reconstructed_texts.append(reconstruct_text(i, prompt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abf1156c-c694-4218-af20-72f020356857",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df = pd.DataFrame({\"original_text\":texts[300:], \"reconstructed_text\":reconstructed_texts})\n",
    "result_df.to_csv('/projects/anra7539/projects/representation_efficacy/reconstructed_from_amrs_qllama3/reconstructed_texts_300+.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c267f2e1-ce3b-439b-b7d3-9615478e684b",
   "metadata": {},
   "source": [
    "## Reconstruction results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "be0c6ce3-c706-41b1-bac7-c3f6d4a3b3e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "files = os.listdir('/projects/anra7539/projects/representation_efficacy/reconstructed_from_amrs_qllama3/')\n",
    "files.remove('.ipynb_checkpoints')\n",
    "data = []\n",
    "for file in files:\n",
    "    data.append(pd.read_csv('/projects/anra7539/projects/representation_efficacy/reconstructed_from_amrs_qllama3/'+file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "22401c54-1de5-4772-b768-dc63edcec415",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_reconstructions = pd.concat(data, ignore_index = True)[['original_text', 'reconstructed_text']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4675dcc-7ba9-4405-8d1c-6aafa91a0f1b",
   "metadata": {},
   "source": [
    "### Average F1-score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b056e1fc-b955-412e-baf6-ff59fcf7429a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def f1_score_strings(str1, str2):\n",
    "    tokens1 = set(str1.lower().split())\n",
    "    tokens2 = set(str2.lower().split())\n",
    "    \n",
    "    true_positives = len(tokens1 & tokens2)  \n",
    "    false_positives = len(tokens1 - tokens2)  \n",
    "    false_negatives = len(tokens2 - tokens1)  \n",
    "    \n",
    "    precision = true_positives / (true_positives + false_positives) if true_positives + false_positives > 0 else 0\n",
    "    recall = true_positives / (true_positives + false_negatives) if true_positives + false_negatives > 0 else 0\n",
    "    \n",
    "    if precision + recall > 0:\n",
    "        f1 = 2 * (precision * recall) / (precision + recall)\n",
    "    else:\n",
    "        f1 = 0\n",
    "    \n",
    "    return f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3587f2d1-4d11-43d5-96fb-babc4cccb0e6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "full_reconstructions['f1_scores'] = full_reconstructions.apply(lambda x: f1_score_strings(x['original_text'], x['reconstructed_text']), axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "38e2ba77-d76e-41de-8448-7abbceb822ac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.41747350277633133"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(full_reconstructions.f1_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2efd25c2-c0f4-4484-86f8-7db943082f75",
   "metadata": {},
   "source": [
    "### Sentence Transformer embedding similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7157d725-ddd9-4e5e-9cc6-47ca3962206c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/projects/anra7539/software/anaconda/envs/kgenv/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:1617: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be deprecated in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "similarity_model = sentence_transformers.SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "\n",
    "def sent_similarity(str1, str2):\n",
    "    embedding1 = similarity_model.encode(str1)\n",
    "    embedding2 = similarity_model.encode(str2)\n",
    "    \n",
    "    return sentence_transformers.util.cos_sim(embedding1, embedding2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0091baea-35e7-4980-9f82-4abf8d6f73ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_reconstructions['cosine_similarity'] = full_reconstructions.apply(lambda x: sent_similarity(x['original_text'], x['reconstructed_text']), axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "65705c72-561e-4d60-8bb5-03a565f4ab43",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7805601693493311"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(full_reconstructions.cosine_similarity)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7aabc554-7ad8-4b58-9da3-41f96552df8f",
   "metadata": {},
   "source": [
    "### ROUGE scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4be3be4d-5a97-48e3-9914-bed63d7d2f4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rouge_score import rouge_scorer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7bacbb2f-06b7-49d6-9c0e-cf5dccef30dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_rouge_scores(reference_text, generated_text):\n",
    "    scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
    "    scores = scorer.score(reference_text, generated_text)\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e817e6b4-94c0-4af4-9054-8f3904b10471",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_reconstructions['rouge_scores'] = full_reconstructions.apply(lambda x: compute_rouge_scores(x['original_text'], x['reconstructed_text']), axis = 1)\n",
    "full_reconstructions['rouge_1'] = full_reconstructions.rouge_scores.map(lambda x: x['rouge1'].fmeasure)\n",
    "full_reconstructions['rouge_2'] = full_reconstructions.rouge_scores.map(lambda x: x['rouge2'].fmeasure)\n",
    "full_reconstructions['rouge_l'] = full_reconstructions.rouge_scores.map(lambda x: x['rougeL'].fmeasure)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c5335ea3-d190-41e9-b4b6-9916ac67929e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROUGE-1 score = 0.5850877843608724\n",
      "ROUGE-2 score = 0.23063268408945545\n",
      "ROUGE-L score = 0.42714075456098805\n"
     ]
    }
   ],
   "source": [
    "print(f\"ROUGE-1 score = {np.mean(full_reconstructions.rouge_1)}\")\n",
    "print(f\"ROUGE-2 score = {np.mean(full_reconstructions.rouge_2)}\")\n",
    "print(f\"ROUGE-L score = {np.mean(full_reconstructions.rouge_l)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f18ca33-2e51-44fc-b381-586e9797ac1a",
   "metadata": {},
   "source": [
    "## BLEU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "624d1f0f-a207-43bb-8d98-b024c914d145",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.translate.bleu_score import sentence_bleu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e50c417e-12f9-4956-9707-3615e2c85558",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/projects/anra7539/software/anaconda/envs/kgenv/lib/python3.11/site-packages/nltk/translate/bleu_score.py:552: UserWarning: \n",
      "The hypothesis contains 0 counts of 4-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "/projects/anra7539/software/anaconda/envs/kgenv/lib/python3.11/site-packages/nltk/translate/bleu_score.py:552: UserWarning: \n",
      "The hypothesis contains 0 counts of 3-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "/projects/anra7539/software/anaconda/envs/kgenv/lib/python3.11/site-packages/nltk/translate/bleu_score.py:552: UserWarning: \n",
      "The hypothesis contains 0 counts of 2-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n"
     ]
    }
   ],
   "source": [
    "full_reconstructions['bleu_scores'] = full_reconstructions.apply(lambda x: sentence_bleu([nltk.word_tokenize(x['original_text'].lower())], \n",
    "                                                                                         nltk.word_tokenize(x['reconstructed_text'].lower())), axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "964f97d3-6cbc-4a38-b405-ee1d0b8c9a87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.08681297802999965\n"
     ]
    }
   ],
   "source": [
    "print(np.mean(full_reconstructions.bleu_scores))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
