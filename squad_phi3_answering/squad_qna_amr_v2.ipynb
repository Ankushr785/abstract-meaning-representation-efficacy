{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7a71cf96-1fd1-4fd8-ad44-c95a51b228e0",
   "metadata": {},
   "source": [
    "## SQuAD v2 0-shot QA using Phi-3-mini-128k-instruct with AMR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "39e09bb9-3199-484f-a53a-b02210a567a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "import torch\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import sentence_transformers\n",
    "import nltk\n",
    "import datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4ff5fd93-e453-4665-ac1e-56f59bfd0743",
   "metadata": {},
   "outputs": [],
   "source": [
    "squad = datasets.load_dataset(\"squad_v2\")\n",
    "squad_val = squad['validation'].to_pandas()\n",
    "squad_val['answer_list'] = squad_val.answers.map(lambda x: x['text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b75784fc-88ef-4b67-9645-26c13e77ebb3",
   "metadata": {},
   "source": [
    "Processing AMR file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8c40f65-5c8b-4116-8fdf-85ff92593e65",
   "metadata": {},
   "outputs": [],
   "source": [
    "squad_amrs = pd.read_csv(\"/projects/anra7539/projects/representation_efficacy/squad_val_amrs.csv\")\n",
    "\n",
    "squad_amrs.drop_duplicates(inplace = True)\n",
    "squad_amrs.reset_index(drop = True, inplace = True)\n",
    "\n",
    "squad_amrs['only_amr'] = squad_amrs.amr.map(lambda x: \"\\n\".join(x.split(\"\\n\")[1:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "944863c9-a1d3-47d5-a4e5-19bb2151e53b",
   "metadata": {},
   "outputs": [],
   "source": [
    "squad_val = squad_val.merge(squad_amrs, on = ['context'], how = 'right').reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8acaed9c-2456-40e5-8047-8624d959e907",
   "metadata": {},
   "source": [
    "### Inferencing pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "26542ec9-e32e-4a32-84a9-d466101759f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d69ce37cd8f34edbb65321ba25edb32f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "name = \"microsoft/Phi-3-mini-128k-instruct\"\n",
    "device = \"cuda\"\n",
    "\n",
    "model = transformers.AutoModelForCausalLM.from_pretrained(name,\n",
    "                                                          load_in_8bit = True,\n",
    "                                                          trust_remote_code = True,\n",
    "                                             device_map = device,\n",
    "                                             cache_dir='/scratch/alpine/anra7539')\n",
    "\n",
    "tokenizer = transformers.AutoTokenizer.from_pretrained(name, truncation_side = \"left\")\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.pad_token_id = tokenizer.eos_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "350fe0a8-0229-4d3e-b896-53bd09d10e16",
   "metadata": {},
   "outputs": [],
   "source": [
    "def qna(question, context, amr, prompt):\n",
    "    with torch.no_grad():\n",
    "        input_text = f'''{prompt}\\n\\nContext:{context}\\n\\nAMR:{amr}\\n\\nQuestion:{question}\\nAnswer:'''\n",
    "        input_tokens = tokenizer(input_text, return_tensors = \"pt\", truncation = True, max_length = 2048).to(device)\n",
    "    \n",
    "        outputs = model.generate(**input_tokens, max_new_tokens = 30, pad_token_id = tokenizer.eos_token_id)\n",
    "    \n",
    "        answer = tokenizer.decode(outputs[0], skip_special_tokens = True).split(\"Answer:\")[1].split(\"\\n\")[0].split(\".\")[0].strip()\n",
    "    return answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2f7a41cf-b2c1-4ece-96d1-98066fe754d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = f'''Answer the given question based on the context.\n",
    "If the question can't be answered based on the information in the context, return \"unanswerable\".\n",
    "You will not return anything except the answer.\n",
    "You may also use the provided linearized Abstract Meaning Representation (AMR) structure of the paragraph to aid in reasoning.'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3563dfa9-6696-4a17-a0ed-c62745951224",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_file = '/projects/anra7539/projects/representation_efficacy/squad_amr_answers_qphi3/predicted_answers.json'\n",
    "\n",
    "if os.path.exists(output_file):\n",
    "    with open(output_file, 'r') as f:\n",
    "        try:\n",
    "            existing_data = [json.loads(line) for line in f]\n",
    "        except json.JSONDecodeError:\n",
    "            existing_data = []\n",
    "else:\n",
    "    existing_data = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47c987e8-8341-4cca-b587-e1211bf04444",
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_indices = {item['index'] for item in existing_data}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0838c606-f393-47d3-b384-5bdde909bcd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(output_file, 'a') as f:\n",
    "    for i in tqdm(range(len(squad_val))):\n",
    "        if i in processed_indices:\n",
    "            continue \n",
    "        \n",
    "        answer = qna(squad_val.question[i], squad_val.context[i], squad_val.only_amr[i], prompt)\n",
    "        \n",
    "        result = {\n",
    "            \"index\": i,\n",
    "            \"context\": squad_val.context[i],\n",
    "            \"question\": squad_val.question[i],\n",
    "            \"answer_list\": list(squad_val.answer_list[i]),\n",
    "            \"prediction\": answer\n",
    "        }\n",
    "\n",
    "        f.write(json.dumps(result) + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bf690c3-05ad-45ca-825a-14e6cc70a628",
   "metadata": {},
   "source": [
    "## Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea194872-8b06-449d-89da-bbbe622a800c",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_file = '/projects/anra7539/projects/representation_efficacy/squad_raw_answers_qphi3/predicted_answers.json'\n",
    "\n",
    "with open(output_file, 'r') as f:\n",
    "    data = [json.loads(line) for line in f]\n",
    "\n",
    "full_dataset = pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f982595c-b4a6-4234-b4a5-d714c53bd839",
   "metadata": {},
   "source": [
    "### Average F1-score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "abda4152-a429-45ef-a7d7-f112a1c91f14",
   "metadata": {},
   "outputs": [],
   "source": [
    "def f1_score_strings(str1, str2):\n",
    "    tokens1 = set(str1.lower().split())\n",
    "    tokens2 = set(str2.lower().split())\n",
    "    \n",
    "    true_positives = len(tokens1 & tokens2)  \n",
    "    false_positives = len(tokens1 - tokens2)  \n",
    "    false_negatives = len(tokens2 - tokens1)  \n",
    "    \n",
    "    precision = true_positives / (true_positives + false_positives) if true_positives + false_positives > 0 else 0\n",
    "    recall = true_positives / (true_positives + false_negatives) if true_positives + false_negatives > 0 else 0\n",
    "    \n",
    "    if precision + recall > 0:\n",
    "        f1 = 2 * (precision * recall) / (precision + recall)\n",
    "    else:\n",
    "        f1 = 0\n",
    "    \n",
    "    return f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b119c4d1-de20-4015-aaca-09073f3616f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def max_score_extraction(tgt_string, ref_string, scoring_function):\n",
    "    score = []\n",
    "    if len(ref_string)>0:\n",
    "        for s in ref_string:\n",
    "            score.append(scoring_function(tgt_string, s))\n",
    "\n",
    "        return max(score)\n",
    "    else:\n",
    "        return scoring_function(tgt_string, \"unanswerable\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "671a3e64-2982-48f5-a104-1ee926146e72",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_dataset.fillna('', inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "af8dd416-9176-4e3d-b8f4-8e19540f125f",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_dataset['f1_scores'] = full_dataset.apply(lambda x: max_score_extraction(x['prediction'], x['answer_list'], f1_score_strings), axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "54f58475-1bcc-4396-885a-7a44dc3d607e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6157322038293058"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(full_dataset.f1_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74f67164-0784-4c3d-acfb-0feac5c02e22",
   "metadata": {},
   "source": [
    "## Cosine similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f651bc7b-51a5-4c98-9337-45c527624a3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "similarity_model = sentence_transformers.SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "\n",
    "def sent_similarity(str1, str2):\n",
    "    embedding1 = similarity_model.encode(str1.lower())\n",
    "    embedding2 = similarity_model.encode(str2.lower())\n",
    "    \n",
    "    return sentence_transformers.util.cos_sim(embedding1, embedding2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "186f6e32-1f1f-4183-8807-a0178e006003",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_dataset['cosine_similarity'] = full_dataset.apply(lambda x: max_score_extraction(x['prediction'], x['answer_list'], sent_similarity), axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4c45f4e3-81be-42f9-a023-94bb7b7dfbae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6854046125137762"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(full_dataset.cosine_similarity)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bab2665-998e-4ed0-b130-5a844cc04906",
   "metadata": {},
   "source": [
    "## ROUGE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d7d5f91f-bdcf-4221-a952-ed9eae2e0f77",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rouge_score import rouge_scorer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b840ac58-a904-4797-86a0-2ece2f0b104d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_rouge_1(generated_text, reference_text):\n",
    "    scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
    "    scores = scorer.score(reference_text.lower(), generated_text.lower())\n",
    "    return scores['rouge1'].fmeasure\n",
    "\n",
    "def compute_rouge_2(generated_text, reference_text):\n",
    "    scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
    "    scores = scorer.score(reference_text.lower(), generated_text.lower())\n",
    "    return scores['rouge2'].fmeasure\n",
    "\n",
    "def compute_rouge_l(generated_text, reference_text):\n",
    "    scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
    "    scores = scorer.score(reference_text.lower(), generated_text.lower())\n",
    "    return scores['rougeL'].fmeasure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "48759858-0496-4ad3-a600-403c818b44bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_dataset['rouge_1'] = full_dataset.apply(lambda x: max_score_extraction(x['prediction'], x['answer_list'], compute_rouge_1), axis = 1)\n",
    "full_dataset['rouge_2'] = full_dataset.apply(lambda x: max_score_extraction(x['prediction'], x['answer_list'], compute_rouge_2), axis = 1)\n",
    "full_dataset['rouge_L'] = full_dataset.apply(lambda x: max_score_extraction(x['prediction'], x['answer_list'], compute_rouge_l), axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b052b7ed-d0e3-4ac6-a45d-913e288c8bf4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROUGE-1 score = 0.6356323669653294\n",
      "ROUGE-2 score = 0.5172571385455257\n",
      "ROUGE-L score = 0.6343967849515131\n"
     ]
    }
   ],
   "source": [
    "print(f\"ROUGE-1 score = {np.mean(full_dataset.rouge_1)}\")\n",
    "print(f\"ROUGE-2 score = {np.mean(full_dataset.rouge_2)}\")\n",
    "print(f\"ROUGE-L score = {np.mean(full_dataset.rouge_L)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0903c03a-f709-45ba-9beb-0b7698f53a54",
   "metadata": {},
   "source": [
    "## BLEU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "413cfd7d-7e18-46a0-9418-d1d0aebcce4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.translate.bleu_score import sentence_bleu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "764e7060-780c-4ec3-b855-c7fd8535f1a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def max_score_extraction_bleu(tgt_string, ref_string, sentence_bleu):\n",
    "    score = []\n",
    "    if len(ref_string)>0:\n",
    "        for s in ref_string:\n",
    "            score.append(sentence_bleu([nltk.word_tokenize(tgt_string.lower())], \n",
    "                                       nltk.word_tokenize(s.lower())))\n",
    "\n",
    "        return max(score)\n",
    "    else:\n",
    "        return sentence_bleu([nltk.word_tokenize(tgt_string.lower())], \n",
    "                             nltk.word_tokenize(\"unanswerable\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ca79c289-c4c2-4c45-b720-2ffde6195c86",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/projects/anra7539/software/anaconda/envs/kgenv/lib/python3.11/site-packages/nltk/translate/bleu_score.py:552: UserWarning: \n",
      "The hypothesis contains 0 counts of 2-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "/projects/anra7539/software/anaconda/envs/kgenv/lib/python3.11/site-packages/nltk/translate/bleu_score.py:552: UserWarning: \n",
      "The hypothesis contains 0 counts of 3-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "/projects/anra7539/software/anaconda/envs/kgenv/lib/python3.11/site-packages/nltk/translate/bleu_score.py:552: UserWarning: \n",
      "The hypothesis contains 0 counts of 4-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n"
     ]
    }
   ],
   "source": [
    "full_dataset['bleu_scores'] = full_dataset.apply(lambda x: max_score_extraction_bleu(x['prediction'], x['answer_list'], sentence_bleu), axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "3b28c4db-a65f-49f1-810f-a1e40b768e9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.07049791420245992\n"
     ]
    }
   ],
   "source": [
    "print(np.mean(full_dataset.bleu_scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db1125dc-9130-435f-abe2-0ada820b3560",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
