{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3439bce3-2a36-47c9-8134-c2624c160288",
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "import torch\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import sentence_transformers\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e928822a-88b7-4e8e-9a21-e1733e9dabe2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.device_count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "21357afe-4d26-475f-9b80-3110318246fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('amr-release-3.0-amrs-bolt.txt', \"r\") as f:\n",
    "  data = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fd8c3847-eb10-4cf7-90dc-906c18559342",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_amr_pairs = data.split(\"::snt\")[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d77bd3bb-4c24-4fc4-83dd-6efb73011ae6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1327/1327 [00:00<00:00, 514878.95it/s]\n"
     ]
    }
   ],
   "source": [
    "texts = []\n",
    "amrs = []\n",
    "\n",
    "for i in tqdm(range(len(text_amr_pairs))):\n",
    "    try:\n",
    "        texts.append(text_amr_pairs[i].split(\"\\n#\")[0].strip())\n",
    "        amrs.append(text_amr_pairs[i].split(\"\\n#\")[1].split(\".txt\\n\")[1].strip())\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "979a5b3f-f1ad-41a7-9812-a5ad96dd4ed8",
   "metadata": {},
   "outputs": [],
   "source": [
    "name = \"microsoft/Phi-3-mini-128k-instruct\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "92dae0cc-aef0-4aff-84c0-aa733b46937a",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/projects/anra7539/projects/representation_efficacy/bolt_texts.txt', 'w') as file:\n",
    "    file.write(\"\\n\".join(texts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a7ba7594-a273-42e1-83e4-a39ef0d41cb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/projects/anra7539/projects/representation_efficacy/bolt_amrs.txt', 'w') as file:\n",
    "    file.write(\"\\n\".join(amrs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b0c7cf2a-a919-440a-8a9a-66d284b488b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`flash-attention` package not found, consider installing for better performance: No module named 'flash_attn'.\n",
      "Current `flash-attention` does not support `window_size`. Either upgrade or use `attn_implementation='eager'`.\n",
      "The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ede9693c51d84b43a78f71909c370e66",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "device = \"cuda\"\n",
    "\n",
    "model = transformers.AutoModelForCausalLM.from_pretrained(name,\n",
    "                                                          load_in_8bit = True,\n",
    "                                                          trust_remote_code = True,\n",
    "                                                          torch_dtype = torch.bfloat16,\n",
    "                                             device_map = device,\n",
    "                                             cache_dir='/scratch/alpine/anra7539')\n",
    "\n",
    "tokenizer = transformers.AutoTokenizer.from_pretrained(name, padding_side=\"left\", truncation_side = \"left\")\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.pad_token_id = tokenizer.eos_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f61e32a3-788c-4be7-b911-bf56be19413c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reconstruct_text(amr, prompt):\n",
    "    with torch.no_grad():\n",
    "        input_text = f'''{prompt}\\nAMR:{amr}\\nText:'''\n",
    "        input_tokens = tokenizer(input_text, return_tensors = \"pt\", truncation = True, max_length = 4096).to(device)\n",
    "    \n",
    "        outputs = model.generate(**input_tokens, max_new_tokens = 200, pad_token_id = tokenizer.eos_token_id)\n",
    "    \n",
    "        answer = tokenizer.decode(outputs[0], skip_special_tokens = True).split(\"Text:\")[6].split(\"\\n\")[0].strip()\n",
    "    return answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "71ca369e-2832-4ad0-9730-3c45a1980028",
   "metadata": {},
   "outputs": [],
   "source": [
    "few_shot_examples = np.random.randint(1000, size = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7a99b1aa-c547-4367-85dd-fa154d624658",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "examples = \"\\n\".join([f\"AMR:{amrs[i]}\\nText:{texts[i]}\" for i in few_shot_examples])\n",
    "\n",
    "prompt = f'''Based on the 5 examples below, return the original text of the given AMR:\\n\\n{examples}'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5600c769-30e9-4ef1-a70b-971b3872348a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1227 [00:00<?, ?it/s]The `seen_tokens` attribute is deprecated and will be removed in v4.41. Use the `cache_position` model input instead.\n",
      "You are not running the flash-attention implementation, expect numerical differences.\n",
      "/projects/anra7539/software/anaconda/envs/kgenv/lib/python3.11/site-packages/bitsandbytes/autograd/_functions.py:316: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      " 97%|█████████▋| 1191/1227 [5:57:35<10:46, 17.95s/it]  "
     ]
    }
   ],
   "source": [
    "reconstructed_texts = []\n",
    "for i in tqdm(amrs[100:]):\n",
    "    reconstructed_texts.append(reconstruct_text(i, prompt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abf1156c-c694-4218-af20-72f020356857",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df = pd.DataFrame({\"original_text\":texts[100:], \"reconstructed_text\":reconstructed_texts})\n",
    "result_df.to_csv('/projects/anra7539/projects/representation_efficacy/reconstructed_from_amrs_qphi3/reconstructed_texts_100+.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c267f2e1-ce3b-439b-b7d3-9615478e684b",
   "metadata": {},
   "source": [
    "## Reconstruction results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "be0c6ce3-c706-41b1-bac7-c3f6d4a3b3e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "files = os.listdir('/projects/anra7539/projects/representation_efficacy/reconstructed_from_amrs_qphi3/')\n",
    "files.remove('.ipynb_checkpoints')\n",
    "data = []\n",
    "for file in files:\n",
    "    data.append(pd.read_csv('/projects/anra7539/projects/representation_efficacy/reconstructed_from_amrs_qphi3/'+file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "22401c54-1de5-4772-b768-dc63edcec415",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_reconstructions = pd.concat(data, ignore_index = True)[['original_text', 'reconstructed_text']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ff760ab-7f92-417e-ad8f-f2b6d7a93f4f",
   "metadata": {},
   "source": [
    "## F1-score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b056e1fc-b955-412e-baf6-ff59fcf7429a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def f1_score_strings(str1, str2):\n",
    "    tokens1 = set(str1.lower().split())\n",
    "    tokens2 = set(str2.lower().split())\n",
    "    \n",
    "    true_positives = len(tokens1 & tokens2)  \n",
    "    false_positives = len(tokens1 - tokens2)  \n",
    "    false_negatives = len(tokens2 - tokens1)  \n",
    "    \n",
    "    precision = true_positives / (true_positives + false_positives) if true_positives + false_positives > 0 else 0\n",
    "    recall = true_positives / (true_positives + false_negatives) if true_positives + false_negatives > 0 else 0\n",
    "    \n",
    "    if precision + recall > 0:\n",
    "        f1 = 2 * (precision * recall) / (precision + recall)\n",
    "    else:\n",
    "        f1 = 0\n",
    "    \n",
    "    return f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3587f2d1-4d11-43d5-96fb-babc4cccb0e6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "full_reconstructions['f1_scores'] = full_reconstructions.apply(lambda x: f1_score_strings(x['original_text'], x['reconstructed_text']), axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "38e2ba77-d76e-41de-8448-7abbceb822ac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3828041715328612"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(full_reconstructions.f1_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23d0ba1d-1ae5-4477-82ce-2c672749bc87",
   "metadata": {},
   "source": [
    "## Cosine similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "dd5d01ab-3c19-4720-ad67-65e9c9155b47",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/projects/anra7539/software/anaconda/envs/kgenv/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:1617: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be deprecated in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "similarity_model = sentence_transformers.SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "\n",
    "def sent_similarity(str1, str2):\n",
    "    embedding1 = similarity_model.encode(str1)\n",
    "    embedding2 = similarity_model.encode(str2)\n",
    "    \n",
    "    return sentence_transformers.util.cos_sim(embedding1, embedding2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "79115b4e-55dc-477d-8e97-d248e1619ab5",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_reconstructions['cosine_similarity'] = full_reconstructions.apply(lambda x: sent_similarity(x['original_text'], x['reconstructed_text']), axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "96f51b32-abee-4b4b-af92-4daad6526d31",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7317269849166352"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(full_reconstructions.cosine_similarity)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f68844a-ac0b-466f-b93f-3cc63caa08b8",
   "metadata": {},
   "source": [
    "## ROUGE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "38c57cd6-8bbf-45c0-b931-3e7514cb4a08",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rouge_score import rouge_scorer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fe2b1fed-5ee5-4a16-82c7-72bf7df8a8ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_rouge_scores(reference_text, generated_text):\n",
    "    scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
    "    scores = scorer.score(reference_text, generated_text)\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ed7149dc-4b67-4022-bd89-4a44e539c1df",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_reconstructions['rouge_scores'] = full_reconstructions.apply(lambda x: compute_rouge_scores(x['original_text'], x['reconstructed_text']), axis = 1)\n",
    "full_reconstructions['rouge_1'] = full_reconstructions.rouge_scores.map(lambda x: x['rouge1'].fmeasure)\n",
    "full_reconstructions['rouge_2'] = full_reconstructions.rouge_scores.map(lambda x: x['rouge2'].fmeasure)\n",
    "full_reconstructions['rouge_l'] = full_reconstructions.rouge_scores.map(lambda x: x['rougeL'].fmeasure)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "32eda505-7e0c-4c38-8c71-a13666c8b595",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROUGE-1 score = 0.530909282558982\n",
      "ROUGE-2 score = 0.1971222258446021\n",
      "ROUGE-L score = 0.38959139451117264\n"
     ]
    }
   ],
   "source": [
    "print(f\"ROUGE-1 score = {np.mean(full_reconstructions.rouge_1)}\")\n",
    "print(f\"ROUGE-2 score = {np.mean(full_reconstructions.rouge_2)}\")\n",
    "print(f\"ROUGE-L score = {np.mean(full_reconstructions.rouge_l)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72d2b763-1c4e-4525-b281-4c7f4dc0881b",
   "metadata": {},
   "source": [
    "## BLEU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5a520bc7-9eaa-41ad-a715-35d7059105c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.translate.bleu_score import sentence_bleu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f3c34b53-476f-4fe7-b075-9af911c7433a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/projects/anra7539/software/anaconda/envs/kgenv/lib/python3.11/site-packages/nltk/translate/bleu_score.py:552: UserWarning: \n",
      "The hypothesis contains 0 counts of 2-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "/projects/anra7539/software/anaconda/envs/kgenv/lib/python3.11/site-packages/nltk/translate/bleu_score.py:552: UserWarning: \n",
      "The hypothesis contains 0 counts of 3-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "/projects/anra7539/software/anaconda/envs/kgenv/lib/python3.11/site-packages/nltk/translate/bleu_score.py:552: UserWarning: \n",
      "The hypothesis contains 0 counts of 4-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n"
     ]
    }
   ],
   "source": [
    "full_reconstructions['bleu_scores'] = full_reconstructions.apply(lambda x: sentence_bleu([nltk.word_tokenize(x['original_text'].lower())], \n",
    "                                                                                         nltk.word_tokenize(x['reconstructed_text'].lower())), axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6e4c88e8-fccc-47d0-a1f2-94a4b631a832",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.06565243238945787\n"
     ]
    }
   ],
   "source": [
    "print(np.mean(full_reconstructions.bleu_scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58aa0bfb-77c5-4e4e-8437-5625a3a98dc6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
